{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7b9fba-4d5e-4da7-9785-fad3aa2e2701",
   "metadata": {},
   "source": [
    "# **Self-Attention Attribution: Interpreting Information Interactions Inside Transformer**\n",
    "\n",
    "## **1️ Introduction**\n",
    "Transformers use **self-attention mechanisms** to capture dependencies between words in a sentence. However, interpreting these attention mechanisms is **challenging** because attention scores do not directly indicate importance.\n",
    "\n",
    "This paper introduces **Self-Attention Attribution (ATTATTR)**, an **Integrated Gradients-based** method to:\n",
    "- **Attribute importance** to self-attention connections.\n",
    "- **Interpret information flow** between token pairs.\n",
    "- **Prune redundant attention heads** while preserving model performance.\n",
    "\n",
    "# **2️ Comparison: Normal Integrated Gradients (IG) vs. Self-Attention Attribution (ATTATTR)**\n",
    "\n",
    "## **- Example Sentence**\n",
    "Consider the sentence:\n",
    "\n",
    "> `\"This paper introduces a new interpretation method.\"`\n",
    "\n",
    "Let's analyze how **Normal IG** and **Self-Attention Attribution (ATTATTR)** attribute importance.\n",
    "\n",
    "---\n",
    "\n",
    "## **- Normal Integrated Gradients (IG)**\n",
    "\n",
    "- **Measures feature importance** at the **token level**.\n",
    "- Computes **gradients w.r.t. input embeddings**.\n",
    "\n",
    "### **Example: Normal IG at Token Level**\n",
    "| Token   | IG Attribution |\n",
    "|---------|--------------|\n",
    "| **This** | 0.21 |\n",
    "| **paper** | 0.35 |\n",
    "| **introduces** | 0.25 |\n",
    "| **a** | 0.05 |\n",
    "| **new** | 0.08 |\n",
    "| **interpretation** | 0.29 |\n",
    "| **method** | 0.12 |\n",
    "\n",
    " **Key Limitation:** Normal IG **ignores token interactions** (e.g., how \"paper\" interacts with \"introduces\").\n",
    "\n",
    "---\n",
    "\n",
    "## **- Self-Attention Attribution (ATTATTR)**\n",
    "- **Extends IG to token pairs** by computing gradients w.r.t. **attention scores** instead of input tokens.\n",
    "- Uses **self-attention matrices** as the feature of interest.\n",
    "- **Interpolates between**:\n",
    "  - A **zero attention baseline**.\n",
    "  - The **actual attention scores**.\n",
    "\n",
    "### **Example: Self-Attention Attribution at Token-Pair Level**\n",
    "| Token 1 | Token 2 | Attention Attribution |\n",
    "|---------|---------|----------------------|\n",
    "| **This** | **paper** | 0.42 |\n",
    "| **paper** | **introduces** | 0.51 |\n",
    "| **introduces** | **a** | 0.12 |\n",
    "| **a** | **new** | 0.07 |\n",
    "| **new** | **interpretation** | 0.38 |\n",
    "| **interpretation** | **method** | 0.22 |\n",
    "\n",
    "**Key Advantage:**  \n",
    "- **Captures interactions** (e.g., \"paper\" → \"introduces\" has strong attribution) which is helpfull for better interpretting\n",
    "\n",
    "---\n",
    "\n",
    " **This makes ATTATTR more useful for interpreting attention mechanisms in Transformers!**\n",
    "## **3️ How ATTATTR Works**\n",
    "### **- Step 1: Define Self-Attention Attribution**\n",
    "Self-Attention Attribution extends **Integrated Gradients (IG)** to measure the **importance of attention scores** rather than individual token embeddings.\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attr}_h(A) = A_h \\circ \\int_{0}^{1} \\frac{\\partial F(\\alpha A)}{\\partial A_h} d\\alpha \\in \\mathbb{R}^{n \\times n}\n",
    "$$\n",
    "\n",
    "\n",
    "This equation follows the **Integrated Gradients approach**, but instead of working with token embeddings, it operates on **attention matrices**.\n",
    "\n",
    "\n",
    "### **- Step 2: Compute Integrated Gradients for Attention**\n",
    "\n",
    "This measures **how much each attention score contributes** to the final prediction.\n",
    "\n",
    "### **- Step 3: Aggregate Importance Across Layers**\n",
    "The final **self-attention attribution** is obtained by **summing across layers** and normalizing.\n",
    "\n",
    "---\n",
    "## **4️ Approach I Used**\n",
    "- I used a pretained  **BERT model and I fine-tuned it to SST-2** for 2 epochs.\n",
    "- Sentences are tokenized and passed through the model to extract self-attention scores.\n",
    "- Now we compute IG all along the path for 20 steps where we compute the gradient of interpolated attention wrt to actual layer attention and intergate and scale it.\n",
    "- This gives attribution of all 144 heads with respect to each pair and we take the max from each head.\n",
    "- This method allows us to identify the most influential attention heads that contribute to the model’s final decision.\n",
    "- In my approach for pruning I used top 3 atributions meaning the top 3 heads from each layer which contribute for the binary classification in SST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74581c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8420' max='8420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8420/8420 16:19, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.171700</td>\n",
       "      <td>0.241471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.309403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./fine-tuned-bert-sst2\\\\tokenizer_config.json',\n",
       " './fine-tuned-bert-sst2\\\\special_tokens_map.json',\n",
       " './fine-tuned-bert-sst2\\\\vocab.txt',\n",
       " './fine-tuned-bert-sst2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load SST-2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-sst2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,  #  epochs \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-bert-sst2\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-bert-sst2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af519a73-9463-4e68-9fb6-478af29f716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Max Attributions Per Layer ###\n",
      "Layer 0: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.2838859220209997e-07, 4.099202612906083e-07, 6.938620913388149e-07, 4.86448016090435e-07, 1.8767563858546055e-07, 2.583869616046286e-07, 2.3428418671755935e-07, 2.577722568730678e-07, 1.7206644997713738e-07, 3.7211111703072675e-07, 9.305368848799844e-07, 2.52074244144751e-07]]\n",
      "Layer 1: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [8.030790468183113e-07, 5.438438961391512e-07, 8.766593850850768e-07, 7.747558470327931e-07, 3.8774999211454997e-07, 3.084090280935925e-07, 4.3068607169516326e-07, 8.884110798135225e-07, 6.532380893986556e-07, 6.083054131522658e-07, 3.7765150295854255e-07, 7.169562650233274e-07]]\n",
      "Layer 2: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0132379202332231e-06, 7.592911970277783e-07, 9.163374556919734e-07, 7.523110525653465e-07, 6.035265300852188e-07, 3.212982448985713e-07, 3.440103739649203e-07, 5.155666826794914e-07, 6.680824071736424e-07, 1.0132753232028335e-06, 8.174764616342145e-07, 3.5591932601164444e-07]]\n",
      "Layer 3: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.259180847540847e-07, 2.5750924237399886e-07, 5.302932208905986e-07, 3.350608892560558e-07, 5.223231482887059e-07, 8.3105493331459e-07, 4.3078523503936594e-07, 4.0270708723255666e-07, 6.231554152691388e-07, 9.966576044462272e-07, 3.1382941756419314e-07, 3.754879571715719e-07]]\n",
      "Layer 4: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [8.239575777224672e-07, 3.445896084031119e-07, 8.048877475630434e-07, 9.964350056179683e-07, 9.346505862595222e-07, 6.661121005890891e-07, 5.460402121570951e-07, 7.370466619249783e-07, 8.800366799732728e-07, 5.794647108814388e-07, 7.880269095039694e-07, 4.819663672606112e-07]]\n",
      "Layer 5: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.555186083303852e-07, 9.499426596448757e-07, 7.928711625027063e-07, 3.635306029536878e-07, 7.200675895546738e-07, 9.652585504227318e-07, 7.377615816039906e-07, 9.393760365128401e-07, 3.640516865743848e-07, 8.142063734339899e-07, 7.184045216490631e-07, 4.321122446526715e-07]]\n",
      "Layer 6: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [9.79980200099817e-07, 7.680541784793604e-07, 3.9586444700034917e-07, 6.249487114473595e-07, 7.375969062195509e-07, 6.36767481410061e-07, 6.313655944722996e-07, 5.80887842716038e-07, 8.064471330726519e-07, 5.575674890678783e-07, 8.48575723466638e-07, 6.130725296316086e-07]]\n",
      "Layer 7: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [5.924243282606767e-07, 3.578846019536286e-07, 8.215575348913262e-07, 7.26970768027968e-07, 4.6420839794336644e-07, 3.664159748950624e-07, 6.36374636542314e-07, 7.828305115253897e-07, 5.230267561273649e-07, 6.200841085046704e-07, 8.547966672267648e-07, 5.668294988936395e-07]]\n",
      "Layer 8: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [4.6787889118604653e-07, 4.971728344571602e-07, 4.6727296876269975e-07, 3.2580743436483317e-07, 8.438071290584048e-07, 6.808332386754046e-07, 7.536743282798852e-07, 5.29833641849109e-07, 6.173933684294752e-07, 3.379292081717722e-07, 6.163958801153058e-07, 6.458571988332551e-07]]\n",
      "Layer 9: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [8.543688636564184e-07, 3.2129290161719837e-07, 2.3306249374854815e-07, 4.2566318825265625e-07, 2.416372808511369e-07, 3.9260862649825867e-07, 6.709317972308781e-07, 9.217436627295683e-07, 4.966723849975097e-07, 3.089758706664725e-07, 1.888380296577452e-07, 8.905313961804495e-07]]\n",
      "Layer 10: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [2.550727344896586e-07, 3.5932640685132355e-07, 6.719845941915992e-07, 9.210085636368603e-07, 6.130268843662634e-07, 3.3511889796500327e-07, 4.827198267776112e-07, 7.448921337527281e-07, 2.564978274222085e-07, 7.739774900983321e-07, 4.748010837829497e-07, 2.4218886096605274e-07]]\n",
      "Layer 11: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.6231375354891497e-07, 1.7388013873187447e-07, 1.766975685768557e-07, 4.996898610443168e-07, 2.494905402272707e-07, 1.5875551184763026e-07, 3.1116076115722535e-07, 3.852919405744615e-07, 2.0513529364052374e-07, 1.429739597824664e-07, 1.8076104879583e-07, 2.7384723466639116e-07]]\n",
      "\n",
      "### Keeping Only Top 3 Heads Per Layer ###\n",
      "Layer 0, Input 0: Keeping Only Heads [0, 1, 2]\n",
      "Layer 0, Input 1: Keeping Only Heads [0, 1, 2]\n",
      "Layer 0, Input 2: Keeping Only Heads [0, 1, 2]\n",
      "Layer 0, Input 3: Keeping Only Heads [10, 2, 3]\n",
      "Layer 1, Input 0: Keeping Only Heads [0, 1, 2]\n",
      "Layer 1, Input 1: Keeping Only Heads [0, 1, 2]\n",
      "Layer 1, Input 2: Keeping Only Heads [0, 1, 2]\n",
      "Layer 1, Input 3: Keeping Only Heads [7, 2, 0]\n",
      "Layer 2, Input 0: Keeping Only Heads [0, 1, 2]\n",
      "Layer 2, Input 1: Keeping Only Heads [0, 1, 2]\n",
      "Layer 2, Input 2: Keeping Only Heads [0, 1, 2]\n",
      "Layer 2, Input 3: Keeping Only Heads [9, 0, 2]\n",
      "Layer 3, Input 0: Keeping Only Heads [0, 1, 2]\n",
      "Layer 3, Input 1: Keeping Only Heads [0, 1, 2]\n",
      "Layer 3, Input 2: Keeping Only Heads [0, 1, 2]\n",
      "Layer 3, Input 3: Keeping Only Heads [9, 5, 0]\n",
      "Layer 4, Input 0: Keeping Only Heads [0, 1, 2]\n",
      "Layer 4, Input 1: Keeping Only Heads [0, 1, 2]\n",
      "Layer 4, Input 2: Keeping Only Heads [0, 1, 2]\n",
      "Layer 4, Input 3: Keeping Only Heads [3, 4, 8]\n",
      "Layer 5, Input 0: Keeping Only Heads [0, 1, 2]\n",
      "Layer 5, Input 1: Keeping Only Heads [0, 1, 2]\n",
      "Layer 5, Input 2: Keeping Only Heads [0, 1, 2]\n",
      "Layer 5, Input 3: Keeping Only Heads [5, 1, 7]\n",
      "Layer 6, Input 0: Keeping Only Heads [0, 1, 2]\n",
      "Layer 6, Input 1: Keeping Only Heads [0, 1, 2]\n",
      "Layer 6, Input 2: Keeping Only Heads [0, 1, 2]\n",
      "Layer 6, Input 3: Keeping Only Heads [0, 10, 8]\n",
      "Layer 7, Input 0: Keeping Only Heads [0, 1, 2]\n",
      "Layer 7, Input 1: Keeping Only Heads [0, 1, 2]\n",
      "Layer 7, Input 2: Keeping Only Heads [0, 1, 2]\n",
      "Layer 7, Input 3: Keeping Only Heads [10, 2, 7]\n",
      "Layer 8, Input 0: Keeping Only Heads [0, 1, 2]\n",
      "Layer 8, Input 1: Keeping Only Heads [0, 1, 2]\n",
      "Layer 8, Input 2: Keeping Only Heads [0, 1, 2]\n",
      "Layer 8, Input 3: Keeping Only Heads [4, 6, 5]\n",
      "Layer 9, Input 0: Keeping Only Heads [0, 1, 2]\n",
      "Layer 9, Input 1: Keeping Only Heads [0, 1, 2]\n",
      "Layer 9, Input 2: Keeping Only Heads [0, 1, 2]\n",
      "Layer 9, Input 3: Keeping Only Heads [7, 11, 0]\n",
      "Layer 10, Input 0: Keeping Only Heads [0, 1, 2]\n",
      "Layer 10, Input 1: Keeping Only Heads [0, 1, 2]\n",
      "Layer 10, Input 2: Keeping Only Heads [0, 1, 2]\n",
      "Layer 10, Input 3: Keeping Only Heads [3, 9, 7]\n",
      "Layer 11, Input 0: Keeping Only Heads [0, 1, 2]\n",
      "Layer 11, Input 1: Keeping Only Heads [0, 1, 2]\n",
      "Layer 11, Input 2: Keeping Only Heads [0, 1, 2]\n",
      "Layer 11, Input 3: Keeping Only Heads [3, 7, 6]\n",
      "\n",
      "Predictions Before Pruning: [0, 1, 0, 1]\n",
      "Predictions After Pruning: [0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model_path = \"./fine-tuned-bert-sst2\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, output_attentions=True, output_hidden_states=True)\n",
    "model.eval()\n",
    "\n",
    "sentences = [\n",
    "    \"The plot was interesting but the execution was poor.\",\n",
    "    \"Absolutely stunning visuals with no real story.\",\n",
    "    \"I will never watch this garbage again!\",\n",
    "    \"I can't believe how fantastic this movie was!\",\n",
    "]\n",
    "\n",
    "\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "inputs_embeds = model.bert.embeddings.word_embeddings(input_ids).detach()\n",
    "inputs_embeds.requires_grad = True \n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs_embeds=inputs_embeds, output_attentions=True)\n",
    "    original_attentions = torch.stack(outputs.attentions)  \n",
    "\n",
    "num_steps = 20\n",
    "integrated_gradients = torch.zeros_like(inputs_embeds)\n",
    "\n",
    "for alpha in torch.linspace(0, 1, num_steps):\n",
    "    interpolated_attention = (alpha * original_attentions).detach().clone()  \n",
    "    interpolated_embeds.requires_grad = True\n",
    "\n",
    "    interpolated_outputs = model(inputs_embeds=inputs_embeds, output_attentions=True)\n",
    "    loss = interpolated_outputs.logits.max()\n",
    "\n",
    "    gradients = torch.autograd.grad(loss, inputs_embeds, retain_graph=True)[0]\n",
    "    integrated_gradients += gradients / num_steps  \n",
    "\n",
    "expanded_integrated_gradients = integrated_gradients.sum(dim=-1, keepdim=True)\n",
    "expanded_integrated_gradients = expanded_integrated_gradients.unsqueeze(0).unsqueeze(2).expand_as(original_attentions)\n",
    "\n",
    "attribution_scores = original_attentions * expanded_integrated_gradients\n",
    "num_layers, batch_size, num_heads = attribution_scores.shape[:3]\n",
    "max_attributions_per_layer = attribution_scores.max(dim=-1)[0].max(dim=-1)[0]\n",
    "top_heads_per_layer = [\n",
    "    [torch.argsort(max_attributions_per_layer[layer_idx, batch_idx], descending=True)[:3].tolist()\n",
    "     for batch_idx in range(batch_size)]\n",
    "    for layer_idx in range(num_layers)\n",
    "]\n",
    "print(\"\\n### Max Attributions Per Layer ###\")\n",
    "for layer_idx in range(num_layers):\n",
    "    print(f\"Layer {layer_idx}: {max_attributions_per_layer[layer_idx].tolist()}\")\n",
    "print(\"\\n### Keeping Only Top 3 Heads Per Layer ###\")\n",
    "for layer_idx, batch_heads in enumerate(top_heads_per_layer):\n",
    "    for batch_idx, heads in enumerate(batch_heads):\n",
    "        print(f\"Layer {layer_idx}, Input {batch_idx}: Keeping Only Heads {heads}\")\n",
    "\n",
    "pruned_attributions = torch.zeros_like(attribution_scores)\n",
    "for layer_idx in range(num_layers):\n",
    "    for batch_idx in range(batch_size):\n",
    "        mask = torch.zeros(num_heads, device=attribution_scores.device)\n",
    "        mask[top_heads_per_layer[layer_idx][batch_idx]] = 1  # Keep only top heads\n",
    "        pruned_attributions[layer_idx, batch_idx] = attribution_scores[layer_idx, batch_idx] * mask[:, None, None]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_after = model(input_ids=input_ids)\n",
    "    preds_after = torch.argmax(outputs_after.logits, dim=1)\n",
    "\n",
    "print(f\"\\nPredictions Before Pruning: {outputs.logits.argmax(dim=1).tolist()}\")\n",
    "print(f\"Predictions After Pruning: {preds_after.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ffe5d-e962-4684-ae61-21411b54de19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
